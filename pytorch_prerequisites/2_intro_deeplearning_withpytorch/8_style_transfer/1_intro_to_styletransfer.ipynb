{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e5a47d",
   "metadata": {},
   "source": [
    "# Introduction to Style Transfer\n",
    "\n",
    "You have seen that **Convolutional Neural Networks (CNNs)** are among the most powerful networks for image classification and analysis. CNNs process visual information in a feed-forward manner, passing an input image through a collection of image filters which extract certain features.\n",
    "\n",
    "## Beyond Classification\n",
    "It turns out that these feature-level representations are not only useful for classification but also for **image construction**. These representations form the basis for creative applications such as:\n",
    "*   **Style Transfer**\n",
    "*   **Deep Dream**\n",
    "\n",
    "These algorithms compose new images based on CNN layer activations and the features extracted during processing.\n",
    "\n",
    "## What is Style Transfer?\n",
    "In this lesson, we will focus on learning about and implementing the **Style Transfer** algorithm. Style transfer allows you to apply the artistic style of one image to another image of your choice.\n",
    "\n",
    "The key to this technique is using a trained CNN to **separate the content from the style** of an image:\n",
    "*   **Content**: The objects and arrangement in the image (e.g., a cat).\n",
    "*   **Style**: The artistic texture and color palette (e.g., Hokusai's *The Great Wave*).\n",
    "\n",
    "If you can successfully separate these components, you can merge the *content* of one image with the *style* of another to create something entirely unique.\n",
    "\n",
    "![Input image transformed into multiple filtered images](image1.png)\n",
    "\n",
    "By the end of this lesson, you will have the knowledge needed to separate style and content to generate stylized images of your own design.\n",
    "\n",
    "# Separating Style & Content\n",
    "\n",
    "When a CNN is trained for image classification, its layers learn to extract increasingly complex features. As we go deeper into the network, pooling layers discard detailed spatial information (like precise pixel locations) to focus on the semantic **content** of the image.\n",
    "\n",
    "## Content Representation\n",
    "Later layers in a CNN are often referred to as the **content representation** because they capture the high-level arrangement and identity of objects, ignoring specific textures or colors.\n",
    "\n",
    "![Style Representation in CNN Layers](image2.png)\n",
    "\n",
    "## Style Representation\n",
    "But what about style? **Style** refers to traits like brush strokes, texture, colors, and curvature. To separate style from content, we need to look at the \"feature space\" designed to capture texture and color information.\n",
    "\n",
    "This is done by analyzing **spatial correlations** within a layer of the network.\n",
    "*   **Correlation**: A measure of the relationship between feature maps in a specific layer (e.g., the depth of the layer).\n",
    "*   **Feature Maps**: If a specific layer has a depth of 64, it has 64 feature maps. We measure how strongly features in one map relate to features in another.\n",
    "\n",
    "![Similarities Between Feature Maps](image3.png)\n",
    "\n",
    "For example, asking \"Is a certain color in map A always found with a certain edge in map B?\" helps us derive the style. If there are common colors and shapes among feature maps, this collective information defines the **image's style**.\n",
    "\n",
    "## Combining Them\n",
    "Style Transfer works by taking two images:\n",
    "1.  **Content Image**: Provides the objects and arrangement (from deep CNN layers).\n",
    "2.  **Style Image**: Provides the color, texture, and patterns (from correlations in feature maps).\n",
    "\n",
    "The algorithm merges these to create a new third image that renders the original content using the artistic style of the second image.\n",
    "\n",
    "# VGG19 & Content Loss\n",
    "\n",
    "In this implementation, we follow the method outlined in the paper [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) by Gatys et al.\n",
    "\n",
    "## The VGG19 Network\n",
    "We use a pre-trained **VGG19** network, a 19-layer CNN. We rely on it not for classification, but as a **feature extractor**.\n",
    "*   It consists of stacks of convolutional layers followed by pooling layers.\n",
    "*   The depth of layers increases after each pooling operation.\n",
    "*   We can access specific layers by their stack and order (e.g., `Conv1_1`, `Conv5_4`).\n",
    "\n",
    "![VGG-19 Architecture](image4.png)\n",
    "![VGG-19 Architecture with Labels](image5.png)\n",
    "\n",
    "## The Goal\n",
    "Both the **Content Image** and the **Style Image** are passed through VGG19.\n",
    "*   **Content Image** path: Extracts representations from deep layers (content).\n",
    "*   **Style Image** path: Extracts features from multiple layers (style).\n",
    "\n",
    "We then create a **Target Image** (starting as a blank canvas or a copy of the content image) and iteratively update it so that:\n",
    "1.  Its content matches the content image.\n",
    "2.  Its style matches the style image.\n",
    "\n",
    "## Content Loss\n",
    "To make the target image look like the content image, we calculate the **Content Loss**.\n",
    "\n",
    "We take the output from a specific layer (in the paper, **Conv4_2** is used) for both the target and content images.\n",
    "*   $C_c$: Content representation of the Content Image.\n",
    "*   $T_c$: Content representation of the Target Image.\n",
    "\n",
    "![Content Image Processing](image6.png)\n",
    "\n",
    "The Content Loss ($L_{content}$) is the Mean Squared Error (MSE) between these two representations.\n",
    "\n",
    "![Content Loss Formula](image8.png)\n",
    "\n",
    "Our goal during the \"training\" (which actually updates the image pixels, not the network weights) is to **minimize this loss**. By doing so, the target image evolves to structurally resemble the content image.\n",
    "\n",
    "# Gram Matrix\n",
    "\n",
    "To capture **Style**, we look at correlations between features in the network's layers. We typically calculate this for multiple layers (from `conv1_1` up to `conv5_1`) to get a **multiscale style representation**.\n",
    "\n",
    "The mathematical tool used to calculate these correlations is the **Gram Matrix**.\n",
    "\n",
    "## Calculating the Gram Matrix\n",
    "Let's see an example with a single layer:\n",
    "1.  **Feature Maps**: Imagine a layer that has 8 feature maps (depth=8), and the spatial dimensions are 4x4.\n",
    "    ![Input Image to Convolutional Layer](image10.png)\n",
    "2.  **Vectorize**: To find correlations, we first **vectorize** (flatten) each feature map. A 4x4 map becomes a vector of length 16.\n",
    "    ![One Feature Map](image11.png)\n",
    "    ![Vectorized Feature Map](image12.png)\n",
    "3.  **Multiply**: We treat the flattened layer as a matrix (8 rows, 16 columns) and multiply it by its **transpose**.\n",
    "    ![Gram Matrix Calculation](image13.png)\n",
    "4.  **Result**: The result is a square matrix (8x8) called the Gram Matrix.\n",
    "\n",
    "## Why the Gram Matrix?\n",
    "*   **Non-localized Information**: By flattening the spatial dimensions (X, Y), we lose information about *where* things are in the image.\n",
    "*   **Style Texture**: What remains is information about *what* features typically appear together (e.g., texture, prominence of colors) regardless of position.\n",
    "\n",
    "The values in the Gram Matrix ($G$) indicate similarities: $G_{4,2}$ tells us how similar the 4th and 2nd feature maps are. This effectively represents the \"style\" of that layer.\n",
    "\n",
    "# Style Loss\n",
    "\n",
    "Once we have the Gram Matrices, we can calculate the **Style Loss**.\n",
    "\n",
    "We calculate the mean squared distance between the Gram matrices of the Style Image ($S_s$) and the Target Image ($T_s$). This is done for all five layers (pairs) we selected (`conv1_1` through `conv5_1`).\n",
    "\n",
    "## Style Weights\n",
    "We don't just add them up equally. We assign **Style Weights** ($w_i$) to each layer.\n",
    "*   This allows us to control how much effect each layer's style representation has on the final image.\n",
    "*   Earlier layers capture simpler patterns (colors, textures), while deeper layers capture more complex structures.\n",
    "\n",
    "![Style Loss Equation](image14.png)\n",
    "\n",
    "The Total Style Loss ($L_{style}$) is the weighted sum of the losses from each layer.\n",
    "\n",
    "## Total Loss & Optimization\n",
    "Finally, to create our masterpiece, we combine both losses:\n",
    "\n",
    "$$ \\text{Total Loss} = \\alpha L_{content} + \\beta L_{style} $$\n",
    "\n",
    "*   **Content Loss**: Ensures the structure looks like the original photo.\n",
    "*   **Style Loss**: Ensures the texture and colors look like the artwork.\n",
    "\n",
    "![Total Loss Equation](image15.png)\n",
    "\n",
    "We use standard **backpropagation** and optimization to minimize this total loss. However, instead of updating network weights, we iteratively **update the target image pixel values** until it matches our desired content and style.\n",
    "\n",
    "# Loss Weights\n",
    "\n",
    "Before we start coding, we need to balance the two different losses we've calculated: **Content Loss** and **Style Loss**.\n",
    "\n",
    "Since these losses are calculated differently, their raw values can be very different in scale. If we just added them together, one might overpower the other. To ensure our target image reflects both the content and style fairly, we apply constant weights:\n",
    "*   $\\alpha$ (alpha): Content Weight\n",
    "*   $\\beta$ (beta): Style Weight\n",
    "\n",
    "## Total Loss Equation\n",
    "We multiply each loss by its respective weight and add them to get the total loss.\n",
    "\n",
    "![Total Loss Equation with Weights](image16.png)\n",
    "\n",
    "## The Alpha/Beta Ratio\n",
    "In practice, the Style Loss is often much larger than the Content Loss, so the Style Weight ($\\beta$) is typically set to a much larger value than the Content Weight ($\\alpha$).\n",
    "\n",
    "The balance is often expressed as the ratio $\\alpha / \\beta$.\n",
    "*   **Smaller Ratio** (larger $\\beta$): More stylistic effect.\n",
    "*   **Larger Ratio** (smaller $\\beta$): More content content preservation.\n",
    "\n",
    "For example, a ratio of $1/10$:\n",
    "![Alpha Beta Ratio Example](image17.png)\n",
    "\n",
    "## Effect of Different Ratios\n",
    "Varying this ratio drastically changes the result.\n",
    "*   At $10^{-4}$ (very small ratio), the image is almost entirely style/texture, with the content barely visible.\n",
    "*   At $10^{-1}$ (larger ratio), the content is very clear, with only a mild stylistic overlay.\n",
    "\n",
    "![Comparison of Different Ratios](image18.png)\n",
    "\n",
    "These weights are hyperparameters you can tune to get the exact artistic effect you desire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
