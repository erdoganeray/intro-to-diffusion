{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336627d7",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## Problem: Overfitting\n",
    "\n",
    "**Large coefficients → Overfitting**\n",
    "\n",
    "Complex models may have smaller training errors but perform poorly on new data.\n",
    "\n",
    "## Solution: Penalize Large Weights\n",
    "\n",
    "Modify the error function to punish large weights:\n",
    "\n",
    "**New Error = Old Error + Weight Penalty**\n",
    "\n",
    "---\n",
    "\n",
    "## L1 vs L2 Regularization\n",
    "\n",
    "### L1 (Lasso)\n",
    "$$\\text{Error} = -\\frac{1}{m}\\sum_{i=1}^{m}(1-y_i)\\ln(1-\\hat{y}_i) + y_i\\ln(\\hat{y}_i) + \\lambda(|w_1| + ... + |w_n|)$$\n",
    "\n",
    "### L2 (Ridge)\n",
    "$$\\text{Error} = -\\frac{1}{m}\\sum_{i=1}^{m}(1-y_i)\\ln(1-\\hat{y}_i) + y_i\\ln(\\hat{y}_i) + \\lambda(w_1^2 + ... + w_n^2)$$\n",
    "\n",
    "**λ (lambda):** Controls penalty strength\n",
    "- Large λ → More penalty → Simpler model\n",
    "- Small λ → Less penalty → More complex model\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Which?\n",
    "\n",
    "### L1 Regularization\n",
    "- **Result:** Sparse vectors (many weights → 0)\n",
    "- **Use for:** Feature selection\n",
    "- **Example:** `(1, 0, 0, 1, 0)`\n",
    "\n",
    "### L2 Regularization\n",
    "- **Result:** Small homogeneous weights\n",
    "- **Use for:** Training models (generally better performance)\n",
    "- **Example:** `(0.5, 0.3, -0.2, 0.4, 0.1)`\n",
    "\n",
    "---\n",
    "\n",
    "## Why L1 Creates Sparse Vectors?\n",
    "\n",
    "**Vector (1, 0):**\n",
    "- L1: $|1| + |0| = 1$\n",
    "- L2: $1^2 + 0^2 = 1$\n",
    "\n",
    "**Vector (0.5, 0.5):**\n",
    "- L1: $|0.5| + |0.5| = 1$\n",
    "- L2: $0.5^2 + 0.5^2 = 0.5$\n",
    "\n",
    "L2 prefers `(0.5, 0.5)` over `(1, 0)` because it produces a smaller sum of squares → encourages distributing weights evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b769456",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "## Problem: Dominant Network Parts\n",
    "\n",
    "During training, some parts of the network may develop very large weights and dominate the learning process, while other parts remain undertrained and contribute minimally.\n",
    "\n",
    "## Solution: Dropout\n",
    "\n",
    "**Randomly deactivate nodes during training** to force all parts of the network to learn.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "During each epoch:\n",
    "1. Randomly turn off some nodes (set probability, e.g., 20%)\n",
    "2. Perform forward and backward propagation without those nodes\n",
    "3. Other nodes must \"pick up the slack\" and participate more actively\n",
    "4. Repeat with different random nodes each epoch\n",
    "\n",
    "### Dropout Probability\n",
    "\n",
    "**Parameter:** Probability that each node gets dropped per epoch\n",
    "\n",
    "- **p = 0.2** → Each node has 20% chance of being turned off each epoch\n",
    "- Some nodes may drop more often, others less (randomness)\n",
    "- Over many epochs, all nodes get similar treatment on average\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- Prevents co-adaptation of neurons\n",
    "- Forces network to learn robust features\n",
    "- Reduces overfitting\n",
    "- Widely used and highly effective technique\n",
    "\n",
    "### Note\n",
    "\n",
    "Dropout is only applied **during training**. At inference/prediction time, all nodes are active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba9a37",
   "metadata": {},
   "source": [
    "# Local Minima Problem\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "**Gradient descent** finds the steepest descent direction and takes a step in that direction.\n",
    "\n",
    "### Problem: Getting Stuck\n",
    "\n",
    "In complex loss landscapes (like mountain ranges), gradient descent can get trapped in **local minima**:\n",
    "\n",
    "- Algorithm reaches a point where no direction leads downward\n",
    "- This local minimum may not be the **global minimum** (best solution)\n",
    "- Training gets stuck at suboptimal solution\n",
    "\n",
    "## Solution: Random Restarts\n",
    "\n",
    "### Strategy\n",
    "\n",
    "Start gradient descent from **multiple random initial positions** and run independently.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- Increases probability of finding the global minimum\n",
    "- At minimum, finds a better local minimum\n",
    "- Simple yet effective approach\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Initialize weights randomly at different starting points\n",
    "2. Run gradient descent from each starting point\n",
    "3. Compare final results from all runs\n",
    "4. Select the model with the best performance\n",
    "\n",
    "This technique helps escape poor local minima and improves overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fe84f",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem\n",
    "\n",
    "## The Problem\n",
    "\n",
    "**Sigmoid function** has very flat regions at the extremes:\n",
    "- Derivative ≈ 0 at far left or far right\n",
    "- Small derivatives → tiny weight updates → slow/stuck training\n",
    "\n",
    "### Worse in Deep Networks\n",
    "\n",
    "In multilayer perceptrons, the gradient is the **product of derivatives** along the path:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial a_2} \\cdot ... \\cdot \\frac{\\partial a_n}{\\partial w}$$\n",
    "\n",
    "- Each derivative is small (sigmoid derivatives)\n",
    "- Product of small numbers → **vanishing gradient**\n",
    "- Tiny weight updates → ineffective training\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: Better Activation Functions\n",
    "\n",
    "### 1. Hyperbolic Tangent (tanh)\n",
    "\n",
    "![Tanh Formula](image4.png)\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**Advantages:**\n",
    "- Range: [-1, 1] (vs sigmoid's [0, 1])\n",
    "- Larger derivatives\n",
    "- Significant improvement over sigmoid\n",
    "\n",
    "### 2. ReLU (Rectified Linear Unit)\n",
    "\n",
    "![ReLU Formula](image5.png)\n",
    "\n",
    "$$\\text{relu}(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}$$\n",
    "\n",
    "Or simply: $\\text{relu}(x) = \\max(0, x)$\n",
    "\n",
    "**Advantages:**\n",
    "- Derivative = 1 for positive values (no vanishing!)\n",
    "- Simple and fast to compute\n",
    "- Significantly improves training\n",
    "- Barely breaks linearity yet enables complex non-linear solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "### Network Architecture\n",
    "- **Hidden layers:** Use ReLU (or tanh)\n",
    "- **Output layer (classification):** Use sigmoid for probabilities [0, 1]\n",
    "- **Output layer (regression):** Can use ReLU for continuous value prediction\n",
    "\n",
    "### Why It Works\n",
    "Better activation functions → larger derivatives → meaningful gradient updates → effective training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8534029",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "## Standard Gradient Descent\n",
    "\n",
    "### Process per Epoch\n",
    "1. Take **all data** as input\n",
    "2. Run through entire neural network\n",
    "3. Calculate predictions and error\n",
    "4. Backpropagate to update weights\n",
    "\n",
    "### Problem\n",
    "- Huge matrix computations for large datasets\n",
    "- High memory usage\n",
    "- Slow training (one step per epoch with all data)\n",
    "\n",
    "---\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Key Idea\n",
    "Use **small subsets of data** instead of entire dataset for each step.\n",
    "\n",
    "### Why It Works\n",
    "- Small subset gives good estimate of gradient direction\n",
    "- Not perfect, but fast\n",
    "- Multiple approximate steps > one perfect step\n",
    "\n",
    "### Process\n",
    "\n",
    "**Example:** 24 data points split into 4 batches of 6 points\n",
    "\n",
    "1. **Batch 1:** Run 6 points → calculate error → backpropagate → update weights\n",
    "2. **Batch 2:** Run 6 points → calculate error → backpropagate → update weights\n",
    "3. **Batch 3:** Run 6 points → calculate error → backpropagate → update weights\n",
    "4. **Batch 4:** Run 6 points → calculate error → backpropagate → update weights\n",
    "\n",
    "**Result:** 4 steps taken vs 1 step with standard gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison\n",
    "\n",
    "| Method | Steps per Epoch | Accuracy per Step | Speed | Overall Efficiency |\n",
    "|--------|----------------|-------------------|-------|-------------------|\n",
    "| **Gradient Descent** | 1 (all data) | High | Slow | Low |\n",
    "| **Stochastic GD** | Multiple (batches) | Lower | Fast | **High** |\n",
    "\n",
    "### Key Insight\n",
    "**Many slightly inaccurate steps >> One accurate step**\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of SGD\n",
    "\n",
    "✅ Faster training  \n",
    "✅ Lower memory requirements  \n",
    "✅ More frequent weight updates  \n",
    "✅ Better convergence in practice  \n",
    "✅ Can escape shallow local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa168e1f",
   "metadata": {},
   "source": [
    "# Learning Rate & Momentum\n",
    "\n",
    "## Learning Rate Selection\n",
    "\n",
    "### Too Large\n",
    "- Takes huge steps\n",
    "- Fast initially but **overshoots minimum**\n",
    "- Chaotic, unstable training\n",
    "- May never converge\n",
    "\n",
    "### Too Small\n",
    "- Takes tiny steps\n",
    "- Slow but steady progress\n",
    "- Better chance of reaching local minimum\n",
    "- Very slow training\n",
    "\n",
    "### Rule of Thumb\n",
    "**If your model isn't working → decrease the learning rate**\n",
    "\n",
    "### Best Practice\n",
    "Use **adaptive learning rates** that decrease as model approaches solution\n",
    "- Keras and PyTorch provide built-in schedulers\n",
    "- Start large, decay over time\n",
    "\n",
    "---\n",
    "\n",
    "## Momentum\n",
    "\n",
    "### Problem\n",
    "Standard gradient descent gets stuck in local minima (gradient ≈ 0)\n",
    "\n",
    "### Solution\n",
    "Use **weighted average of previous steps** to maintain direction and \"power through\" local minima\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Instead of just current gradient, consider:\n",
    "- Previous step × 1\n",
    "- Step before × β\n",
    "- Step before that × β²\n",
    "- Step before that × β³\n",
    "- ...\n",
    "\n",
    "**β (beta):** Momentum constant (0 < β < 1)\n",
    "\n",
    "### Effect\n",
    "- Recent steps matter **most**\n",
    "- Older steps matter **less** (exponential decay)\n",
    "- Builds \"velocity\" in consistent directions\n",
    "- Helps escape local minima\n",
    "\n",
    "### Benefits\n",
    "✅ Escapes local minima  \n",
    "✅ Smooths optimization path  \n",
    "✅ Faster convergence  \n",
    "✅ Reduces oscillations  \n",
    "\n",
    "### Trade-off\n",
    "May overshoot global minimum slightly, but works very well in practice\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Technique | Purpose | Key Parameter |\n",
    "|-----------|---------|---------------|\n",
    "| **Learning Rate** | Step size control | α (alpha) |\n",
    "| **Momentum** | Escape local minima | β (beta) |\n",
    "\n",
    "**Combined:** Adaptive learning rate + momentum = robust optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
